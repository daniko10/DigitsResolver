{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-Uok6rrlDLQ"
   },
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import subprocess\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X34aI10BlFUw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2 = nn.Dropout2d(0.25)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "        self.dropout3 = nn.Dropout2d(0.25)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.batch_norm1(self.conv1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.batch_norm2(self.conv2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = F.relu(self.batch_norm3(self.conv3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = CNNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUFwMN7FlHJB"
   },
   "outputs": [],
   "source": [
    "def to_binary(image):\n",
    "    threshold = 0.5\n",
    "    return (image > 0).float()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.Lambda(to_binary)\n",
    "])\n",
    "\n",
    "training_data_mnist = torchvision.datasets.MNIST(root=\"../data\", train=True, transform=transform, download=True)\n",
    "test_data_mnist = torchvision.datasets.MNIST(root=\"../data\", train=False, transform=transform, download=True)\n",
    "\n",
    "training_data_emnist = torchvision.datasets.EMNIST(root=\"../data\", split=\"digits\", train=True, transform=transform, download=True)\n",
    "test_data_emnist = torchvision.datasets.EMNIST(root=\"../data\", split=\"digits\", train=False, transform=transform, download=True)\n",
    "\n",
    "full_dataset_train = ConcatDataset([training_data_mnist, training_data_emnist])\n",
    "full_dataset_test = ConcatDataset([test_data_mnist, test_data_emnist])\n",
    "\n",
    "batch_size = 64\n",
    "data_loader_train = DataLoader(full_dataset_train, batch_size=batch_size, shuffle=True)\n",
    "data_loader_test = DataLoader(full_dataset_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jSErKlMlKlC"
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "gFRDeQoGlMcL",
    "outputId": "cdc3c4b8-619d-4dbb-d45d-180d280f9528"
   },
   "outputs": [],
   "source": [
    "Epochs = 3\n",
    "for epoch in range(Epochs):\n",
    "  training_loss = 0.0\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  for i, data in enumerate(data_loader_train, 0):\n",
    "    inputs, labels = data\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_function(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    training_loss += loss.item()\n",
    "    _, predicted = outputs.max(1)\n",
    "    total += labels.size(0)\n",
    "    correct += predicted.eq(labels).sum().item()\n",
    "    avg_loss = training_loss / (i + 1)\n",
    "    avg_acc = 100. * correct / total\n",
    "  print(f'Training Loss: {avg_loss:.3f} | Training acc: {avg_acc:.3f}', 'for epoch: ', epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJXohrBcmLWY"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn.pth')\n",
    "print(\"Model saved to 'cnn.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2labjVd2mNKw"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for i, (image, label) in enumerate(data_loader_test):\n",
    "      output = model(image)\n",
    "      test_loss += F.nll_loss(output, label, reduction='sum').item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(label.data.view_as(pred)).sum()\n",
    "      plt.imshow(image[0].squeeze(0), cmap=\"gray\")\n",
    "      plt.title(f\"Prediction: {pred[0].item()}, Label: {label[0].item()}\")\n",
    "      plt.axis(\"off\")\n",
    "      plt.show()\n",
    "      time.sleep(3)\n",
    "    test_loss /= len(data_loader_test.dataset)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(data_loader_test.dataset), 100. * correct / len(data_loader_test.dataset)))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
